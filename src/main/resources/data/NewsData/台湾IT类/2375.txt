2017新智元開源·生態AI技術峯會【倒計時4天，點擊「閱讀原文」搶票】新智元和行業領袖英特爾聯合舉辦，中國AI2017開年盛典啓幕在即，三大亮點不容錯過：①BAT領銜，英特爾支持各路人工智能技術領袖齊聚，洞察中國AI軍團佈局；②引爆AI原力，現場參與AI技術論壇頂牛對撞；③人工智能創業家巨星璀璨，看投資領袖預測誰將成爲中國AI獨角獸。

【新智元導讀】何凱明（KaimingHe）是ResNet作者之一、FacebookAI實驗室研究科學家。最近，他的最新研究成果MaskR-CNN公佈，這是一個概念上簡單，靈活，而且通用的對象實例分割框架，在COCO的實例分割，邊界框對象檢測，人物關鍵點檢測3個任務上均優於現有的單一模型。

我們提出一個概念上簡單，靈活，而且通用的對象實例分割框架（objectinstancesegmentation）。我們的方法能有效檢測圖像中的對象，同時爲每個實例生成高質量的分割掩膜（segmentationmask）。我們將該方法稱爲MaskR-CNN，是在FasterR-CNN上的擴展，即在用於邊界框識別的現有分支上添加一個並行的用於預測對象掩膜（objectmask）的分支。

MaskR-CNN的訓練簡單，僅比FasterR-CNN多一點系統開銷，運行速度是5fps。此外，MaskR-CNN很容易推廣到其他任務，例如可以用於在同一個框架中判斷人的姿勢。我們在COCO競賽的3個任務上都得到最佳結果，包括實例分割，邊界框對象檢測，以及人物關鍵點檢測。沒有使用其他技巧，MaskR-CNN在每個任務上都優於現有的單一模型，包括優於COCO2016競賽的獲勝模型。我們希望這個簡單而有效的方法將成爲一個可靠的基準，有助於未來的實例層面識別的研究。我們將會公開相關代碼。

MaskR-CNN在概念上是簡單的：FasterR-CNN對每個候選對象有兩個輸出，即一個類標籤和一個邊界框偏移值。我們在FasterR-CNN上添加了第三個分支，即輸出對象掩膜（objectmask）。因此，MaskR-CNN是一種自然而且直觀的想法。但添加的mask輸出與類輸出和邊界框輸出不同，需要提取對象的更精細的空間佈局。MaskR-CNN的關鍵要素包括pixel-to-pixel對齊，這是Fast/FasterR-CNN主要缺失的一塊。

圖2：在COCO測試集上的MaskR-CNN的結果。這些結果基於ResNet-101，實現了35.7的maskAP，運行速度是5fps。圖中，掩膜（mask）用彩色顯示，也顯示了邊界框，類標籤和置信度。

圖3：Head架構：我們擴展了兩個已有的FasterR-CNN的頭（head）。左圖和右圖分別展示了ResNetC4和FPN主幹的head。可以看到上面增加了一個mask分支。圖中的數字表示空間分辨率和信道，箭頭表示卷積（conv），去卷積（deconv）或全連接層（fc）。

我們對MaskR-CNN和當前state-of-the-art的框架進行了全面的比較。所有實驗都使用COCO數據集。我們使用標準COCO指標，包括AP（超過IoU閾值的平均值），AP50，AP75和APS，APM，APL（不同規模的AP）。除非另有說明，AP使用maskloU進行評估。

我們將MaskR-CNN與表1中的在實例分割任務上是state-of-the-art的方法進行比較。我們的模型的所有實例表現都優於這些模型的基線變體。包括MNC和FCIS，這兩個模型分別是COCO2015和2016競賽中分割任務的冠軍。

MaskR-CNN的輸出結果顯示在圖2和圖4。可以看到，MaskR-CNN即使在有挑戰性的條件上也獲得了良好的效果。圖5比較了我們的MaskR-CNN基線和FCIS+++。FCIS+++在重疊的實例上顯示出系統的僞影（artifacts），表明它在實例分割的這個根本難題上受到挑戰。MaskR-CNN沒有顯示出那樣的僞影。

圖5：FCIS+++（上）vs.MaskR-CNN（下，ResNet-101-FPN）。FCIS在面對重疊對象時顯示出系統的僞影。

表2：MaskR-CNN的ablation。在trainval35k上訓練，在minival上測試，並報告了maskAP，除非另有說明。

我們將MaskR-CNN與表3中的當前state-of-the-art的COCO邊界框對象檢測模型進行比較。結果顯示，即使被訓練的是完整的MaskR-CNN模型，也只有分類輸出和邊界框輸出被用於推理（mask輸出被忽略了）。使用ResNet-101-FPN的MaskR-CNN優於所有當前最先進模型的變體，包括GRMI的單模型變體，這是COCO2016競賽檢測任務的冠軍。使用ResNeXt-101-FPN，MaskR-CNN進一步提升了結果。

表3：在test-dev上對象檢測的單模型結果（邊界框AP），vs當前最優模型。

我們的框架可以很方便地擴展用於人體姿勢估計（HumanPoseEstimation）。我們將關鍵點的位置建模爲one-hotmask，採用MaskR-CNN來預測每個K關鍵點類型（例如左肩，右肘）的Kmask。這個任務證明了MaskR-CNN的靈活性。

圖6：使用MaskR-CNN（ResNet-50-FPN）在COCO測試集上的關鍵點檢測結果，具有同一個模型預測的人物分割掩膜。該模型實現了63.1的關鍵點AP，運行速度爲5fps。

鑑於MaskR-CNN在提取對象邊界框，掩膜，以及關鍵點上都有效，我們期待它成爲其他實例層面任務的有效框架。