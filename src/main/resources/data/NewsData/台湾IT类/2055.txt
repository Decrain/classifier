隨着AlphaGo事件的不斷髮酵，神經網絡成爲時下人工智能產學領域萬衆矚目的研究焦點，也成爲普羅大衆的熱門話題。事實上，神經網絡作爲一種算法模型，很早就已經被廣泛關注和研究，也曾長時間內陷入發展突破的低潮期。不過，在以GeoffreyHinton、YannLeCun和YoshuaBengio爲代表的衆多神經網絡活躍研究者的堅持和努力下，人們對卷積神經網絡的研究得到開拓性進展，深度學習進入大衆視野，神經網絡終於在2006年迎來了復興。

YannLeCun作爲深度學習運動的領軍人物，Facebook人工智能研究院院長兼紐約大學教授，其一舉一動都能引發業界的廣泛關注。這次，由清華大學經濟管理學院發起，清華x-lab、Facebook主辦的主題講座邀請了YannLeCun，作爲《創新與創業：硅谷洞察》學分課程中的第一節公開課的講者，針對深度學習技術的歷史發展進程與人工智能的未來趨勢進行了深入的解析，並分享了一些精彩觀點。

一開場，YannLeCun就通過回顧去年的AlphaGo事件引出今天講座的主題《深度學習與人工智能的未來》，並向在場觀衆拋出了一個問題：人類可以利用大量的樣本去訓練機器，可是機器能夠識別出它從未見過的事物嗎？

帶着這個問題，YannLeCun講述了深度學習的發展過程以及他個人在領域內的探索歷程。

首先，YannLeCun回顧了1957年的感知器以及50年代末期的傳統模式識別模型，並對傳統模式識別、主流現代模式識別以及深度學習特徵提取方式進行了比對；隨後，他又對多層神經網絡、反向傳播算法、卷積神經網絡結構（歸一化——濾波器組——非線性計算——池化）等算法概念進行解析；並且展示了1993年完成的LeNET1Demo。

然而，當時業界仍對神經網絡的未來仍存遲疑態度。YannLeCun舉了一件關於他在貝爾實驗室兩位同事Jackel（LarryJackle現任NVIDIA機器學習顧問，曾在加拿大多輪多的一場NVIDIA的分享會講過這個故事，並親自協助完成了機器之心技術分析師對那場分享會的報道，感興趣的讀者可點擊閱讀原文查看此文章。）和Vapnik的趣事。在1995年的一次晚餐中，Jackel曾經跟Vapnik打賭說，在2000年3月14日之前，人們將會理解大型神經網絡，並給出明確的限定，事實證明，Jackel的想法錯了。而Vapnik打賭認爲2005年3月14日後，沒有人將會使用類似於1995年的那些神經網絡，事實證明，Vapnik也錯了。

當事人LarryJackle現任NVIDIA機器學習顧問，曾在加拿大多輪多的一場NVIDIA的分享會講過這個故事，並親自協助機器之心技術分析師對那場分享會的報道。

當卷積網絡度過瓶頸期並得到人們的認可後，深度卷積網絡開始用於解決各類計算機視覺問題，如目標識別；而隨着網絡深度的不斷增加，產生了VGG、GoogLeNet、ResNet等深度卷積神經網絡結構，它們可以用於圖像識別、語義分割、ADAS等衆多場景。

這裏，YannLeCun特別提到了Facebook提出的通用目標分割框架MaskR-CNN，並展示了它在COCO數據集上的結果。（詳情可見：學界|Facebook新論文提出通用目標分割框架MaskR-CNN：更簡單更靈活表現更好）

在爲在場觀衆帶來全新、深入的深度學習技術解析後，YannLeCun又探討了人工智能領域存在的一個障礙和難點——怎樣使機器獲得「常識」呢？

在人工智能領域，機器是如何跨越這種本質的障礙呢？YannLeCun給出了答案，即機器不僅需要學習、理解這個世界，學習大量的背景知識，還需要感知世界的狀態，更新、記憶並評估世界的狀態，而且還要有推理和計劃的能力。這也就是所謂的「智能&常識=感知+預測模型+記憶+推理和規劃」。

人們由於瞭解這個世界運作原理，所以會擁有常識，可是對於機器呢？它們能否具備所謂的「常識」呢？LeCun舉了幾個例子進行說明。比如說「這個箱子裝不下獎盃，因爲它太大/太小了」這句話，當我們說「太大」時，我們知道「它」是獎盃；而當我們說「太小」時，那「它」就是「箱子」了。

機器是無法憑空具備常識的，它需要一些已知的信息，比如根據空間信息推斷世界的狀態、從過去和現在推斷未來、從現在的狀態推斷過去的事件。那麼，這個過程就涉及預測學習（predictivelearning）這一個概念，也就是從提供的任何信息預測過去、現在以及未來的任何一部分。不過，這是很多人對無監督學習（unsupervisedlearning）的定義。

由此看見，無監督學習和預測學習是十分必要的，也是未來幾年深度學習型領域的巨大挑戰。通常，需要拿來去訓練一個大型學習機器的樣本數量取決於我們要求機器所預測的信息量。你需要機器回答的問題越多，樣本數量就要越大。

「大腦有10的14次方個突觸，我們卻只能活大概10的9次方秒。因此我們的參數比我們所獲得的數據會多得多。這一事實激發了這一思想：既然感知輸入（包括生理上的本體感受）是我們每秒獲取10^5維度約束（10^5dimensionsofconstraint）的唯一地方，那麼，就必須進行大量的無監督學習。」

預測人類提供的標籤，一個價值函數（valuefunction）是不夠的。這裏，YannLeCun用一個生動的比喻解釋了不同機器學習算法進行預測需要多少信息，並展示了2016年VizDoom競賽冠軍使用的來自強化學習的Actor-Critic算法來生成序列的實例。

此處，YannLeCun提到了Sutton所提出的Dyna結構，這是一種集學習、規劃、反應於一身的集成架構，即可以完成「在行動之前實現對腦內設想的嘗試」。

之後，YannLeCun介紹了經典基於模型的最優化控制過程。即利用初始控制序列對世界進行仿真，調整控制序列利用梯度下降法對目標進行最優化，再進行反向傳播。

用一個公式概括了人工智能系統，即：預測+規劃=推理。

智能的本質是預測的能力，要提前進行規劃，我們需要模擬這個世界，然後採取行動以最小化預測損失。

YannLeCun指出，機器能否學習出預測世界的模型是實現重大進展的關鍵。

然後，他也介紹了「根據文本推斷世界的狀態：實體RNN」

儘管監督式卷積網絡已經取得了重大的進展，我們仍需要記憶增強網絡賦予機器進行推論的能力。LeCun幫助我們理解記憶了堆棧增強循環神經網絡。

EntNet是第一種解決了所有20中bAbI任務的模型

在bAbI任務上的端到端記憶網路

在這一部分，YannLeCun對基於能量的無監督學習算法進行了比較詳細的解析。

學習一個能量函數（或稱對比函數），其在數據流形上取低值，在其它地方去高值。

壓低我們想要的輸出的能量，推高其它地方。但我們該怎麼選擇推高哪裏呢？

3.數據點能量的下推（pushdown），在選擇出的點上進行提高

接下來，是關於對抗訓練的介紹，YannLeCun本人對對抗訓練給予高度肯定。對抗訓練（GAN）是改進機器預測能力的一種方式。GAN包括一個生成器、一個判別器，它們可以同時進行學習。

它的難點在於在不確定性下進行預測。

對抗訓練：不確定情況下進行預測的關鍵

除了深度卷積對抗生成網絡(DCGAN)，他還介紹了基於能量的對抗生成網絡（EBGAN）。（詳情可見：學界|YannLeCun最新論文：基於能量的生成對抗網絡（附論文））

在演講的最後，YannLeCun提到了語義分割的視頻預測技術，並展示了時間預測結果。

最後簡單總結一下，Yann在演講中總結了去年人工智能領域的進展，並介紹了監督學習的一些知識點。然後，Yann聚焦於無監督學習。他認爲無監督學習會成爲未來的主流，能解決我們的學習系統難以處理的衆多問題。我們如今正在面臨無監督和預測性前向模型（predictiveforwardmodel）的建立，這也可能會是接下來幾年的挑戰。此外，對抗訓練在未來可能會逐漸扮演更重要的角色，而如今的難題是讓機器學習「常識」。

演講結束後，YannLeCun回答了現場觀衆的問題。他本人對近日騰訊圍棋AI絕藝奪冠一事表示興奮，並坦承看好人工智能在ADAS、醫療領域內的發展。