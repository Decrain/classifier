音頻（audio）對於我們對世界的感知的影響的巨大自然不言而喻。語音（speech）顯然是人們最熟悉的通信方式之一，但環境聲音（sound）也能傳達很多重要的信息。我們可以本能地響應這些背景聲音所創造的語境，比如被突然出現的喧鬧而嚇到、使用音樂作爲一種敘述元素或者在情景喜劇中將笑聲用作一種觀衆提示。

自2009年以來，YouTube就開始爲視頻提供自動生成的字幕了，而這主要是專注於語音轉錄以使YouTube上託管的內容能觸及到更多人。但是，沒有對視頻中背景聲音的類似轉錄，視頻中的很多信息和效果都無法單獨使用語音轉錄獲取。爲了解決這個問題，我們宣佈爲YouTube視頻中的自動字幕增加音效信息，從而讓人們能更大程度地獲取豐富的音頻內容。

在這篇文章中，我們討論了爲這項工作所開發的後端系統——這是輔助功能（Accessibility）、聲音理解（SoundUnderstanding）和YouTube團隊合作的成果，他們使用機器學習（ML）實現了有史第一個YouTube自動音效字幕系統。

點擊「字幕/CC」按鈕查看該音效字幕工作時的效果（只有YouTube可實驗此字幕效果）：

這項應用中使用了一種被稱爲深度神經網絡（DNN）的機器學習技術來解決這個特定的字幕任務挑戰。儘管分析視頻的時域音頻信號來檢測多種背景聲音的過程類似於其它已知的分類問題（比如圖像中的目標檢測），但在產品應用中，該解決方案還面臨着額外的難題。特別是以下方面：當給定任意一段音頻時，我們需要模型要能夠：1）檢測出我們想要的聲音，2）在時間上對該聲音進行定位，3）有效地和可能有並行和獨立的多個語音識別結果的字幕進行整合。

在開發此模型時我們面臨的首個挑戰是要獲得足夠多適合該神經網絡訓練的標記數據。雖然有標記的背景聲音信息很難獲得，但我們能夠使用弱標記數據生成足夠大的數據集來進行訓練。但在給定視頻中的所有背景聲音中，我們該用哪種聲音來訓練用於檢測的DNN呢？

在這個最初發布的功能中，我們選擇了「鼓掌」、「音樂」和「笑聲」，這主要是基於我們對人類創造的字幕的分析，分析表明這些背景聲音是人工添加最多的字幕。雖然在這三種聲音之外，還有遠遠更多的聲音類別能提供遠遠更豐富的相關信息，但字幕中的這些音效所傳遞的語義信息是相對清楚的，比如相比於「鈴聲」字幕——它會引發這樣的問題「這是什麼的鈴聲？鈴鐺、時鐘還是手機？」

最初我們做了不少檢測這些背景聲音的工作，這些工作還包括開發可擴展未來工作的基礎與分析框架，聲音事件的探測，以及其與自動字幕的整合。當我們擴展算法以理解更廣泛的聲音詞彙時，對基礎開發的投資將使我們在未來更容易地把更多的聲音類型(比如[鈴聲]、[敲門聲]、[吠叫聲])包含進來，從而帶來更多獲益。由此，我們將能在敘述中加入被檢測的聲音以爲用戶提供更多相關信息(比如[鋼琴曲]、[粗啞的掌聲])。

當視頻傳到YouTube上時，音效識別流程就會在該視頻的音頻流上運行。DNN會查看音頻短片段並預測該片段是否包含所需要的聲音事件。因爲多個音效可以共同出現，因此我們的模型可以在每個時間步驟（timestep）對每段音效進行預測。然後該片段窗口（segmentwindow）向右滑動（即稍後的時間點），再一次使用該模型進行預測，這樣一步步向後直到結束。這樣我們就會得到一個密集流（densestream），即詞表中的聲音事件以每秒100幀的頻率出現。

當然，該密集預測流（densepredictionstream）並不會直接展露給用戶，因爲不僅顯示密集預測流會導致字幕的閃爍，同時也是因爲許多音效在發生時具有某種程度的時間連續性。例如，「音樂」和「掌聲」通常至少會存在幾秒鐘。爲了結合這種直覺，我們使用了包含ON和OFF兩個狀態的改進維特比算法（Viterbialgorithm）將密集預測流變得平滑一些，其中每個音效的預測段對應於狀態ON。下圖是從密集檢測到包含了目標音效的最後音頻段這一過程的說明。

左圖：來自我們的用於視頻中單個聲音類別的隨時間的出現情況的DNN的密集的概率序列。中圖：基於修改過的Viterbi算法的二值化的片段。右圖：基於持續時間的濾波器移除了持續時間比該類別的預期時間短的片段。

類似這樣的以分類爲基礎的系統當然會存在一些誤差，也需要爲了產品的目標在假正類（falsepositives）和錯失檢測之間尋找平衡。比如，訓練數據集中的弱標籤常常會讓模型混淆可能會一同出現的事件。比如，一個標記爲「笑」的片段常常包含語音和笑聲，在測試數據中，「笑」模型有時會很難區別它們。在我們的系統中，可以根據ON狀態上的時間（比如，無法決定聲音X是否被識別到，除非這個聲音至少持續Y秒）做出進一步的限制，進而將系統性能推至精確度召回曲線上一個預期的點。

一旦對系統暫時的定位聲效表現感到滿意（基於線下評估標準），我們就會面臨以下問題：如何將聲效與語音解釋結合起來，打造一個單獨的聲音字幕，如何（或何時）將聲效信息傳到給用戶才能讓它們變得最有用？

一旦系統能準確檢測和分類視頻中背景聲音，我們就開始尋找將這一信息傳達給觀衆的有效辦法。與我們的用戶體驗（UX）研究團隊合作，我們探索了不同設計選項並在一個定性測試可用性的研究中測試了這些選項。參與者的聽力水平不同，對字幕的要求也不同。我們問了他們很多問題，包括是否提升了他們的整體體驗，是否能夠搞清楚視頻中發生了什麼並能從說明中提取出相關信息，藉此瞭解這些變量的效果如何，比如：

幾乎所有的用戶讚許了被添加的精確音效信息，對此我們並不吃驚。我們還特別關注了該聲音檢測系統的錯誤反饋（當確定了一個聲音其實卻沒有聲音的假正例或沒能檢測到一個音效）。這個結果讓人吃驚：當音效信息錯誤時，在大約一半的情況下它沒使用戶的體驗降低。基於參與者的反饋，原因可能如下：

我們工作是使YouTube視頻自動匹配音效字幕。這個首次展示只是第一步，我們將繼續努力爲豐富的視頻內容匹配自動字幕，以方便那些由於不同方式不同環境而需要字幕的人。我們已經開發了一個框架，使帶有音效的自動字幕更豐富，但完全做好還要更加努力。我們希望這會在社區之中激發進一步的工作和討論，比如，不僅使用自動技術提升字幕效果，也探討使創建者生成的與社區貢獻的字幕更豐富和更好，從而進一步提升用戶的觀看體驗。