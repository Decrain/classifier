我們提出了一個概念上簡單、靈活和通用的用於目標實例分割（objectinstancesegmentation）的框架。我們的方法能夠有效地檢測圖像中的目標，同時還能爲每個實例生成一個高質量的分割掩碼（segmentationmask）。這個方面被稱爲MaskR-CNN，是在FasterR-CNN上的擴展——在其已有的用於邊界框識別的分支上添加了一個並行的用於預測目標掩碼的分支。MaskR-CNN的訓練很簡單，僅比FasterR-CNN多一點計算開銷，運行速度爲5fps。此外，MaskR-CNN可以很容易泛化到其它任務，比如，讓我們可以在同一個框架中估計人類的姿態。我們在COCO難題套件的所有3種任務（track）上都得到了最佳結果，其中包括實例分割、邊界框目標檢測和人物關鍵點檢測（personkeypointdetection）。沒有使用其它的技巧，MaskR-CNN的表現超越了在每個任務上所有已有的單個模型，包括COCO2016挑戰賽的獲勝模型。我們希望我們的簡單又有效的方法能成爲一個堅實的基礎，能幫助簡化實例層面識別的未來研究。我們將會公開相關代碼。

圖2：在COCO測試集上的MaskR-CNN結果。這些結果基於ResNet-101，在5fps的速度下實現了35.7的maskAP。圖上不同的顏色表示不同的掩碼，另外也給出的邊界框、類別和置信度。

圖3：頭架構（HeadArchitecture）：我們延展了兩個已有的FasterR-CNN頭[14,21]。左圖和右圖分別展示了ResNetC4和FPN的主幹（backbone）的頭（head），分別來自[14]和[21]，可以看到上面還增加了一個mask分支。圖中的數字表示空間分辨率和信道，箭頭表示卷積（conv）、去卷積（deconv）或全連接層（fc），具體可以根據情況推斷（卷積會保持空間維度而去卷積會增加它）。除了輸出卷積是1×1之外，其它所有卷積都是3×3，去卷積是2×2，步幅爲2。我們在隱藏層中使用ReLU[24]。左圖中res5表示ResNet的第5階段，爲了簡單起見，我們進行了修改，使第1個卷積層運行在一個7×7RoI上，步幅爲1（而不是如[14]中的14×14，步幅爲2）。右圖中的×4表示4個連續卷積的堆疊。

表1：在COCOtest-dev上的實例分割maskAP。MNC[7]和FCIS[20]分別是COCO2015和2016分割挑戰賽的獲勝模型。沒有添加其它額外的東西，MaskR-CNN的表現超過了更復雜的FCIS+++——其包括多種規模的訓練/測試、水平翻轉測試和OHEM[29]。所有的條目都是單個模型的結果。

表2：MaskR-CNN的分解。我們是在trainval35k上訓練的，在minival上測試的，除非特別指明都報道的是maskAP成績。

表3：在test-dev上目標檢測單個模型的結果（邊界框AP）vs當前最佳。使用ResNet-101-FPN的MaskR-CNN的表現超越了所有之前最佳模型的基本變體（在這些實驗中忽略了maskoutput）。MaskR-CNN在[21]的基礎上獲得的增益得益於對RoIAlign(+1.1APbb)、多任務訓練(+0.9APbb)和ResNeXt-101(+1.6APbb)的使用。

圖6：使用MaskR-CNN（ResNet-50-FPN）在COCO測試上的關鍵點檢測結果，帶有來自於同一個模型的人物分割掩碼。該模型在5fps條件下實現了63.1的關鍵點AP。

表4：在COCOtest-dev上的關鍵點檢測AP。我們的ResNet-50-FPN是以5fps運行的單個模型。CMUPose+++[4]是2016年的比賽獲勝者，其使用了多尺度測試、帶有CPM的後處理[33]和帶有一個目標檢測器的濾波，累加了約5分（在個人通信中闡明的）。†:G-RMI是在COCPplusMPII[1]（2.5萬張圖像）上訓練的，使用了兩個模型（Inception-ResNet-v2+ResNet-101）。因爲它們使用了更多數據，所以這不是與MaskR-CNN的直接對比。